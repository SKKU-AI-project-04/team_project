{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bongy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "file_path = os.path.join(parent_directory, 'aihub', 'dataset.tsv')\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = [line.strip().split('|', 1) for line in f]\n",
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249689, 3000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPRDataset(Dataset):\n",
    "    def __init__(self, questions, passages, tokenizer):\n",
    "        self.passages = passages\n",
    "        self.questions = questions\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.passages)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        passage = self.passages[index]\n",
    "        question = self.questions[index]\n",
    "        return question, passage\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        passages, questions = zip(*batch)\n",
    "        passage_inputs = self.tokenizer.batch_encode_plus(passages, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        question_inputs = self.tokenizer.batch_encode_plus(questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        return question_inputs, passage_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "q_model = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "p_model.to(device)\n",
    "q_model.to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"kykim/bert-kor-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [row[0] for row in data]\n",
    "passages = [row[1] for row in data]\n",
    "\n",
    "train_questions, valid_questions, train_passages, valid_passages = train_test_split(\n",
    "    questions, passages, test_size=0.1\n",
    ")\n",
    "\n",
    "train_dataset = DPRDataset(train_questions, train_passages, tokenizer)\n",
    "valid_dataset = DPRDataset(valid_questions, valid_passages, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=valid_dataset.collate_fn)\n",
    "# input_ids : [bs, 512],    attention_mask : [bs, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56180, 6243)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtm77\\myenv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\mtm77\\myenv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# AdamW 옵티마이저를 초기화하고 하나의 리스트로 합친 파라미터들을 전달\n",
    "p_optimizer = AdamW(p_model.parameters(), lr=2e-5, eps=1e-8)\n",
    "q_optimizer = AdamW(q_model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "p_scheduler = get_linear_schedule_with_warmup(p_optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "q_scheduler = get_linear_schedule_with_warmup(q_optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "inf_loss = float('inf')\n",
    "cnt = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    q_model.train()\n",
    "    p_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total = len(train_dataloader), desc=\"training\", leave = False):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_question, b_passage = batch\n",
    "\n",
    "        p_optimizer.zero_grad()\n",
    "        q_optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            question_v = q_model(**b_question).pooler_output\n",
    "            passage_v = p_model(**b_passage).pooler_output\n",
    "\n",
    "            cosine = torch.matmul(question_v, torch.transpose(passage_v, 0, 1))\n",
    "            cosine = torch.nn.functional.log_softmax(cosine, dim=1)\n",
    "\n",
    "            targets = torch.arange(0, question_v.shape[0]).long().to(device)\n",
    "\n",
    "            loss = torch.nn.functional.nll_loss(cosine, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(p_optimizer)\n",
    "        scaler.step(q_optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        p_scheduler.step()\n",
    "        q_scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'epoch : {epoch+1}/{epochs}, train loss : {total_loss / len(train_dataloader)}')\n",
    "\n",
    "\n",
    "    q_model.eval()\n",
    "    p_model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0.0\n",
    "        for step, batch in tqdm(enumerate(valid_dataloader), total = len(valid_dataloader), desc = \"validing\", leave = False):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_question, b_passage = batch\n",
    "\n",
    "            question_v = q_model(**b_question).pooler_output\n",
    "            passage_v = p_model(**b_passage).pooler_output\n",
    "\n",
    "            cosine = torch.matmul(question_v, torch.transpose(passage_v, 0, 1))\n",
    "            cosine = torch.nn.functional.log_softmax(cosine, dim=1)\n",
    "\n",
    "            targets = torch.arange(0, question_v.shape[0]).long().to(device)\n",
    "\n",
    "            loss = torch.nn.functional.nll_loss(cosine, targets)\n",
    "            valid_loss += loss\n",
    "        valid_loss_result = valid_loss / len(valid_dataloader)\n",
    "        print(f'epoch : {epoch+1}/{epochs}, valid loss : {valid_loss_result}')\n",
    "        if inf_loss > valid_loss_result:\n",
    "            cnt = 0\n",
    "            inf_loss = valid_loss_result\n",
    "            torch.save(q_model.state_dict(), 'DPR_q.pth')\n",
    "            torch.save(p_model.state_dict(), 'DPR_p.pth')\n",
    "        else:\n",
    "            cnt += 1\n",
    "            if cnt > 3:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bongy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "file_path = os.path.join(parent_directory, 'aihub')\n",
    "\n",
    "questions = {}\n",
    "with open(os.path.join(file_path, 'questions.tsv'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('\\t', 1)\n",
    "        qid = parts[0].strip()\n",
    "        question = parts[1].strip()\n",
    "        questions[qid] = question\n",
    "\n",
    "pids = []\n",
    "passages = []\n",
    "with open(os.path.join(file_path, 'collection.tsv'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('||', 1)\n",
    "        pid = parts[0].strip()\n",
    "        passage = parts[1].strip()\n",
    "        pids.append(pid)\n",
    "        passages.append(passage)\n",
    "\n",
    "answers = {}\n",
    "with open(os.path.join(file_path, 'test', 'qrels_test.tsv'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('\\t', 1)\n",
    "        qid = parts[0].strip()\n",
    "        pid = parts[1].strip()\n",
    "        answers[qid] = pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252689, 124535, 124535, 3000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions), len(pids), len(passages), len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(42000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_p_model = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "test_q_model = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "test_p_model.load_state_dict(torch.load('DPR_p.pth'))\n",
    "test_q_model.load_state_dict(torch.load('DPR_q.pth'))\n",
    "\n",
    "test_p_model.to(device)\n",
    "test_q_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"kykim/bert-kor-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dvs = []\n",
    "\n",
    "test_p_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for p in tqdm(passages):\n",
    "        p_input = tokenizer(p, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        p_dv = test_p_model(**p_input).pooler_output\n",
    "        p_dvs.append(p_dv)\n",
    "\n",
    "p_dvs = torch.cat(p_dvs, dim=0)\n",
    "p_dvs = p_dvs.cpu().numpy()\n",
    "print()\n",
    "print(p_dvs.shape)  # (124535, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dvs = []\n",
    "\n",
    "test_q_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for qid, pid in tqdm(answers.items()):\n",
    "        q = questions[qid]\n",
    "        q_input = tokenizer(q, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        q_dv = test_q_model(**q_input).pooler_output\n",
    "        q_dvs.append(q_dv)\n",
    "\n",
    "q_dvs = torch.cat(q_dvs, dim=0)\n",
    "q_dvs = q_dvs.cpu().numpy()\n",
    "print()\n",
    "print(q_dvs.shape)  #(3000, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 768\n",
    "\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "def normalize_vectors(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    vectors_normalized = vectors / norms\n",
    "    return vectors_normalized\n",
    "\n",
    "p_dvs = normalize_vectors(p_dvs)\n",
    "index.add(p_dvs)\n",
    "\n",
    "k=100\n",
    "_, indices = index.search(q_dvs, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall1 = 0\n",
    "recall10 = 0\n",
    "recall20 =0\n",
    "recall100 = 0\n",
    "\n",
    "for idx, (qid, pid) in tqdm(enumerate(answers.items()), total = len(answers), desc = 'testing', leave = False):\n",
    "    for pred in range(k):\n",
    "        if pids[indices[idx][pred]] == pid:\n",
    "            if pred<1:\n",
    "                recall1+=1\n",
    "                recall10+=1\n",
    "                recall20+=1\n",
    "                recall100+=1\n",
    "                break\n",
    "            elif pred<10:\n",
    "                recall10+=1\n",
    "                recall20+=1\n",
    "                recall100+=1\n",
    "                break\n",
    "            elif pred<20:\n",
    "                recall20+=1\n",
    "                recall100+=1\n",
    "                break\n",
    "            elif pred<100:\n",
    "                recall100+=1\n",
    "                break\n",
    "\n",
    "print()\n",
    "print(f'recall@1 : {recall1/len(answers)}')\n",
    "print(f'recall@10 : {recall10/len(answers)}')\n",
    "print(f'recall@20 : {recall20/len(answers)}')\n",
    "print(f'recall@100 : {recall100/len(answers)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
